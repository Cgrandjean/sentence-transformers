{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cgrdj/Documents/Code/sentence-transformers/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/cgrdj/Documents/Code/sentence-transformers/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/cgrdj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "from beir import util, LoggingHandler\n",
    "from beir.retrieval import models as beir_models\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "# from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from sentence_transformers import models, losses, datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as npR\n",
    "from torch import nn, Tensor\n",
    "from typing import Iterable, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertConfig,AutoConfig, AutoTokenizer, AutoModelForCausalLM, PreTrainedModel,AutoModel\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "# from pretrain.arguments import ModelArguments\n",
    "# from pretrain.enhancedDecoder import BertLayerForDecoder\n",
    "from torch import nn\n",
    "from transformers import BertForMaskedLM, AutoModelForMaskedLM\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
    "\n",
    "from MAELoss import MaskedAutoEncoderLoss\n",
    "from MAEDataset import MaskedAutoEncoderDataset\n",
    "\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from transformers.modeling_utils import (\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.models.bert.modeling_bert import BertIntermediate, BertOutput, BertSelfOutput\n",
    "from transformers.utils import (\n",
    "    logging,\n",
    ")\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_MASKED_LM_MAPPING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,do_lower_case=False,clean_up_tokenization_spaces=False,clean_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HF_model=AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded here: /Users/cgrdj/Documents/Code/sentence-transformers/datasets/scifact\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 49403.40it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"scifact\"\n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "data_path = util.download_and_unzip(url, out_dir)\n",
    "print(\"Dataset downloaded here: {}\".format(data_path))\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"train\") # or split = \"train\" or \"dev\"\n",
    "unsupervised_train_data =  list(queries.values())#+[data['title']+' \\n '+data['text'] for data in list(corpus.values())]\n",
    "random.Random(0).shuffle(unsupervised_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_embedding_model = models.Transformer(model_name, max_seq_length=512)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"cls\")\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertLayerForDecoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m         cls_hiddens \u001b[38;5;241m=\u001b[39m last_hidden_states[:, :\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m---> 24\u001b[0m loss\u001b[38;5;241m=\u001b[39m\u001b[43mMaskedAutoEncoderLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mMaskedAutoEncoderLoss.__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLM_head\u001b[38;5;241m=\u001b[39mBertOnlyMLMHead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mBertLayerForDecoder\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39m_init_weights)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertLayerForDecoder' is not defined"
     ]
    }
   ],
   "source": [
    "class MaskedAutoEncoderLoss(nn.Module):\n",
    "    def __init__(self, model: SentenceTransformer):\n",
    "        super(MaskedAutoEncoderLoss, self).__init__()\n",
    "        self.encoder = model[0].auto_model  # This will be the final model used during the inference time.\n",
    "        self.config = model[0].auto_model.config\n",
    "        self.tokenizer = model.tokenizer\n",
    "        self.MLM_head=BertOnlyMLMHead(self.config)\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.decoder = BertLayerForDecoder(self.config)\n",
    "        self.decoder.apply(self.encoder._init_weights)\n",
    "        \n",
    "    def forward_exp(self, sentence_features: Iterable[Dict[str, Tensor]]):\n",
    "        encoder_inputs,decoder_inputs, target_features= tuple(sentence_features)\n",
    "        target_features[encoder_inputs.input_ids != self.tokenizer.mask_token_id] = -100\n",
    "        last_hidden_states=self.encoder(**encoder_inputs)[0]\n",
    "        lm_logits=self.MLM_head(last_hidden_states)\n",
    "        loss = self.cross_entropy(lm_logits.view(-1, self.config.vocab_size),\\\n",
    "                                  target_features.view(-1))\n",
    "\n",
    "        cls_hiddens = last_hidden_states[:, :1]\n",
    "        embeddings=self.encoder.embeddings(input_ids=decoder_inputs)\n",
    "        hiddens = torch.cat([cls_hiddens, embeddings[:, 1:]], dim=1)\n",
    "\n",
    "        decoder_position_ids = self.embeddings.position_ids[:, :decoder_inputs.size(1)]\n",
    "        decoder_position_embeddings = self.embeddings.position_embeddings(decoder_position_ids)  # B L D\n",
    "        query = decoder_position_embeddings + cls_hiddens\n",
    "\n",
    "        matrix_attention_mask = self.encoder.get_extended_attention_mask(\n",
    "            decoder_attention_mask,\n",
    "            decoder_attention_mask.shape,\n",
    "            decoder_attention_mask.device\n",
    "        )\n",
    "\n",
    "        hiddens = self.c_head(query=query,\n",
    "                              key=hiddens,\n",
    "                              value=hiddens,\n",
    "                              attention_mask=matrix_attention_mask)[0]\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "loss=MaskedAutoEncoderLoss(model.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model=model[0].auto_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0394, -0.0294, -0.2240,  ...,  0.1635, -0.0193, -0.0673],\n",
       "         [ 0.8163,  0.3312,  0.0357,  ..., -0.4310,  0.2611, -0.6336],\n",
       "         [-0.3566,  0.3271,  0.1425,  ...,  0.0231,  0.4354, -0.4355],\n",
       "         ...,\n",
       "         [-0.4133,  0.0168,  0.2789,  ...,  0.5137,  0.7683,  0.4260],\n",
       "         [-0.3264, -0.0387,  0.2875,  ...,  0.4052,  0.8615,  0.4296],\n",
       "         [-0.4734,  0.0301,  0.2788,  ...,  0.4350,  0.6838,  0.3353]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.embeddings(\n",
    "            encoder_inputs.input_ids,\n",
    "            encoder_inputs.attention_mask,\n",
    "            # labels=labels,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=bert_model(\n",
    "            encoder_inputs.input_ids,\n",
    "            encoder_inputs.attention_mask,\n",
    "            # labels=labels,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertModel' object has no attribute 'cls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/sentence-transformers/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'cls'"
     ]
    }
   ],
   "source": [
    "bert_model.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.4593, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.forward_exp(tuple([inputs,labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100, -100, 4372, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_text = \"Here is some text to encode\"\n",
    "encoder_text = \"Here is some text to [MASK]\"\n",
    "decoder_text = \"Here [MASK] [MASK] text [MASK] encode\"\n",
    "encoder_inputs = tokenizer(encoder_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "decoder_inputs = tokenizer(decoder_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "labels = tokenizer(labels_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\").input_ids\n",
    "\n",
    "# Vous devez définir les labels ici en fonction de votre stratégie de masquage\n",
    "# Exemple simplifié où on suppose que le token masqué est à la position 5\n",
    "labels[encoder_inputs.input_ids != tokenizer.mask_token_id] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.6928, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "cross_entropy(\n",
    "head(base_model(**inputs)[0]).view(-1, 30522),\n",
    "labels.view(-1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RetroMAEForPretraining(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            bert: BertForMaskedLM,\n",
    "    ):\n",
    "        super(RetroMAEForPretraining, self).__init__()\n",
    "        self.lm = bert\n",
    "\n",
    "        self.decoder_embeddings = self.lm.bert.embeddings\n",
    "        self.c_head = BertLayerForDecoder(bert.config)\n",
    "        self.c_head.apply(self.lm._init_weights)\n",
    "\n",
    "        # self.model_args = model_args\n",
    "\n",
    "    def forward(self,\n",
    "                encoder_input_ids, encoder_attention_mask, encoder_labels,\n",
    "                decoder_input_ids, decoder_attention_mask, decoder_labels):\n",
    "        \n",
    "        lm_out: MaskedLMOutput = self.lm(\n",
    "            encoder_input_ids, encoder_attention_mask,\n",
    "            labels=encoder_labels,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        print(lm_out)\n",
    "        cls_hiddens = lm_out.hidden_states[-1][:, :1]  # B 1 D\n",
    "\n",
    "        decoder_embedding_output = self.decoder_embeddings(input_ids=decoder_input_ids)\n",
    "        hiddens = torch.cat([cls_hiddens, decoder_embedding_output[:, 1:]], dim=1)\n",
    "\n",
    "        decoder_position_ids = self.lm.bert.embeddings.position_ids[:, :decoder_input_ids.size(1)]\n",
    "        decoder_position_embeddings = self.lm.bert.embeddings.position_embeddings(decoder_position_ids)  # B L D\n",
    "        query = decoder_position_embeddings + cls_hiddens\n",
    "\n",
    "        matrix_attention_mask = self.lm.get_extended_attention_mask(\n",
    "            decoder_attention_mask,\n",
    "            decoder_attention_mask.shape,\n",
    "            decoder_attention_mask.device\n",
    "        )\n",
    "\n",
    "        hiddens = self.c_head(query=query,\n",
    "                              key=hiddens,\n",
    "                              value=hiddens,\n",
    "                              attention_mask=matrix_attention_mask)[0]\n",
    "        pred_scores, loss = self.mlm_loss(hiddens, decoder_labels)\n",
    "\n",
    "        return (loss + lm_out.loss,)\n",
    "\n",
    "    def mlm_loss(self, hiddens, labels):\n",
    "        pred_scores = self.lm.cls(hiddens)\n",
    "        masked_lm_loss = self.cross_entropy(\n",
    "            pred_scores.view(-1, self.lm.config.vocab_size),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        return pred_scores, masked_lm_loss\n",
    "\n",
    "    def save_pretrained(self, output_dir: str):\n",
    "        self.lm.save_pretrained(output_dir)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "            cls,\n",
    "            #   model_args: ModelArguments,\n",
    "            *args, **kwargs\n",
    "    ):\n",
    "        hf_model = AutoModelForMaskedLM.from_pretrained(*args, **kwargs)\n",
    "        model = cls(hf_model,\n",
    "                    #  model_args\n",
    "                     )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'logging' has no attribute 'get_logger'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThe codes are modified based on huggingface transformers library.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m logger \u001b[38;5;241m=\u001b[39m \u001b[43mlogging\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logger\u001b[49m(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBertSelfAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, position_embedding_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'logging' has no attribute 'get_logger'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The codes are modified based on huggingface transformers library.\n",
    "'''\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        mixed_query_layer = self.query(query)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(key))\n",
    "            value_layer = self.transpose_for_scores(self.value(value))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(key))\n",
    "            value_layer = self.transpose_for_scores(self.value(value))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = query.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=query.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=query.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: torch.Tensor,\n",
    "            key: torch.Tensor,\n",
    "            value: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        self_outputs = self.self(\n",
    "            query, key, value,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], query)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertLayerForDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: torch.Tensor,\n",
    "            key: torch.Tensor,\n",
    "            value: torch.Tensor,\n",
    "            attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            head_mask: Optional[torch.FloatTensor] = None,\n",
    "            encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "            past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "            output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            query, key, value,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset=MaskedAutoEncoderDataset(unsupervised_train_data,AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D'))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "train_loss = MaskedAutoEncoderLoss(model, decoder_name_or_path=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMae=RetroMAEForPretraining.from_pretrained( pretrained_model_name_or_path=\"huawei-noah/TinyBERT_General_4L_312D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ambulatory', 'blood', 'pressure', 'monitoring', 'is', 'inaccurate', 'at', 'diagnosing', 'hypertension.']\n",
      "[[2572, 28507, 7062], [2668], [3778], [8822], [2003], [24949], [2012], [22939, 26745, 7741], [23760, 29048, 1012]]\n",
      "[[2572, 28507, 7062], [2668], [3778], [8822], [2003], [24949], [2012], [22939, 26745, 103], [23760, 29048, 1012]]\n",
      "['Ambulatory', 'blood', 'pressure', 'monitoring', 'is', 'inaccurate', 'at', 'diagnosing', 'hypertension.']\n",
      "[[2572, 28507, 7062], [2668], [3778], [8822], [2003], [24949], [2012], [22939, 26745, 7741], [23760, 29048, 1012]]\n",
      "[[103, 28507, 103], [103], [103], [8822], [2003], [24949], [2012], [22939, 26745, 7741], [23760, 29048, 103]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ambulatory blood pressure monitoring is inaccurate at diagno[MASK] hypertension.',\n",
       " '[MASK]bula[MASK] [MASK] [MASK] monitoring is inaccurate at diagnosing hypertension[MASK]',\n",
       " 'Ambulatory blood pressure monitoring is inaccurate at diagnosing hypertension.']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2].texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_attention_mask[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RetroMAEForPretraining.forward() missing 5 required positional arguments: 'encoder_attention_mask', 'encoder_labels', 'decoder_input_ids', 'decoder_attention_mask', and 'decoder_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodelMae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mceci est un test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: RetroMAEForPretraining.forward() missing 5 required positional arguments: 'encoder_attention_mask', 'encoder_labels', 'decoder_input_ids', 'decoder_attention_mask', and 'decoder_labels'"
     ]
    }
   ],
   "source": [
    "modelMae(tokenizer(['ceci est un test'],return_tensors='pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of the model checkpoint at gpt2 were not used when initializing GPT2LMHeadModel: ['h.3.ln_1.weight', 'h.3.ln_1.bias', 'h.11.mlp.c_proj.weight', 'h.6.mlp.c_proj.weight', 'h.11.attn.c_proj.weight', 'h.2.attn.c_attn.bias', 'h.8.ln_2.bias', 'h.1.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.5.attn.c_attn.bias', 'h.6.attn.c_proj.bias', 'h.1.attn.c_attn.bias', 'h.10.attn.c_attn.bias', 'h.5.mlp.c_fc.bias', 'h.1.mlp.c_proj.weight', 'h.7.mlp.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.8.mlp.c_proj.bias', 'h.4.attn.c_proj.bias', 'h.3.mlp.c_proj.bias', 'h.8.attn.c_attn.weight', 'h.7.attn.c_attn.weight', 'h.8.attn.bias', 'h.2.ln_2.bias', 'h.11.ln_1.weight', 'h.6.attn.c_attn.weight', 'h.6.mlp.c_fc.bias', 'h.3.mlp.c_fc.weight', 'h.2.mlp.c_proj.weight', 'h.11.attn.c_attn.bias', 'h.5.attn.bias', 'h.3.attn.c_proj.weight', 'h.11.attn.c_proj.bias', 'h.5.ln_2.bias', 'h.8.ln_1.weight', 'h.9.attn.c_proj.weight', 'h.8.attn.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.7.mlp.c_fc.bias', 'h.3.attn.c_attn.weight', 'h.11.attn.bias', 'h.6.ln_1.bias', 'h.11.ln_2.weight', 'h.9.attn.c_attn.bias', 'h.10.ln_2.bias', 'h.6.mlp.c_proj.bias', 'h.6.ln_2.weight', 'h.10.ln_2.weight', 'h.10.attn.c_attn.weight', 'h.11.ln_2.bias', 'h.9.mlp.c_fc.weight', 'h.6.attn.bias', 'h.5.mlp.c_proj.bias', 'h.1.mlp.c_fc.bias', 'h.3.ln_2.weight', 'h.9.attn.c_attn.weight', 'h.11.attn.c_attn.weight', 'h.7.ln_2.bias', 'h.3.mlp.c_proj.weight', 'h.4.attn.bias', 'h.5.attn.c_proj.weight', 'h.2.mlp.c_fc.weight', 'h.1.ln_2.bias', 'h.7.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.4.mlp.c_proj.weight', 'h.11.ln_1.bias', 'h.7.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.7.attn.c_attn.bias', 'h.4.attn.c_attn.bias', 'h.4.attn.c_proj.weight', 'h.8.mlp.c_fc.weight', 'h.5.ln_1.weight', 'h.2.attn.c_proj.bias', 'h.1.attn.c_proj.bias', 'h.6.ln_1.weight', 'h.1.mlp.c_proj.bias', 'h.7.attn.c_proj.weight', 'h.1.attn.bias', 'h.4.ln_1.bias', 'h.6.ln_2.bias', 'h.11.mlp.c_fc.bias', 'h.8.ln_2.weight', 'h.7.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.4.ln_2.weight', 'h.3.attn.c_proj.bias', 'h.1.ln_1.weight', 'h.10.mlp.c_proj.weight', 'h.8.attn.c_proj.bias', 'h.6.attn.c_attn.bias', 'h.5.ln_1.bias', 'h.8.ln_1.bias', 'h.4.mlp.c_fc.weight', 'h.11.mlp.c_proj.bias', 'h.10.mlp.c_fc.weight', 'h.7.ln_1.weight', 'h.7.ln_2.weight', 'h.8.mlp.c_fc.bias', 'h.6.attn.c_proj.weight', 'h.2.mlp.c_fc.bias', 'h.2.attn.bias', 'h.5.attn.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.10.ln_1.bias', 'h.9.ln_1.weight', 'h.7.mlp.c_fc.weight', 'h.10.attn.c_proj.weight', 'h.1.ln_2.weight', 'h.9.ln_1.bias', 'h.3.attn.c_attn.bias', 'h.3.ln_2.bias', 'h.9.mlp.c_proj.bias', 'h.2.ln_2.weight', 'h.5.ln_2.weight', 'h.5.mlp.c_proj.weight', 'h.4.attn.c_attn.weight', 'h.2.attn.c_attn.weight', 'h.2.ln_1.bias', 'h.3.attn.bias', 'h.10.ln_1.weight', 'h.2.mlp.c_proj.bias', 'h.1.attn.c_proj.weight', 'h.6.mlp.c_fc.weight', 'h.9.attn.c_proj.bias', 'h.5.attn.c_attn.weight', 'h.8.mlp.c_proj.weight', 'h.4.ln_2.bias', 'h.8.attn.c_attn.bias', 'h.4.mlp.c_proj.bias', 'h.10.mlp.c_fc.bias', 'h.9.ln_2.weight', 'h.4.mlp.c_fc.bias', 'h.1.attn.c_attn.weight', 'h.7.attn.bias', 'h.9.mlp.c_proj.weight', 'h.10.attn.bias', 'h.5.mlp.c_fc.weight', 'h.1.ln_1.bias', 'h.9.ln_2.bias', 'h.9.attn.bias']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.ln_cross_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.0.ln_cross_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.q_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decoder_config = AutoConfig.from_pretrained('gpt2')\n",
    "decoder_config.is_decoder = True\n",
    "decoder_config.add_cross_attention = True\n",
    "decoder_config.num_hidden_layers=1\n",
    "kwargs_decoder = {\"config\": decoder_config}\n",
    "decoder = AutoModelForCausalLM.from_pretrained('gpt2', **kwargs_decoder)\n",
    "tokenizer=AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_dataset=datasets.DenoisingAutoEncoderDataset(unsupervised_train_data)#,tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, drop_last=True)\n",
    "train_loss = MaskedAutoEncoderLoss(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18847, 20464, 16026, 27781, 14126,  1997,  1050,  1011, 28353,  5886,\n",
      "          2378, 16171,  3459,  8156,  5012,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [ 2512,  2378, 12044,  3512,  3893,  3778, 19536,  2003,  2025, 16014,\n",
      "          3512,  1997, 11325, 16464,  4945,  2044,  5024,  5812, 22291,  3370,\n",
      "          1012,   102,     0,     0,     0,     0],\n",
      "        [ 3806, 12412, 13697,  3351,  2003,  2019,  4621,  3949,  2005, 11325,\n",
      "         11498, 16211,  2102, 16149,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [13822,  1997,  1042, 23518,  5648,  1006,  6904,  1007,  1998, 17663,\n",
      "          1038,  2575,  1006,  1058,  2497,  2575,  1007, 13416,  3798,  1997,\n",
      "         24004,  5666,  8602,  2063,  1012,   102]]) tensor([[  101, 18847, 20464, 16026, 27781, 14126,  1997,  1050,  1011, 28353,\n",
      "          5886,  2378, 16171,  3459,  8156,  5012,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2512,  2378, 12044,  3512,  3893,  3778, 19536,  2003,  2025,\n",
      "         16014,  3512,  1997, 11325, 16464,  4945,  2044,  5024,  5812, 22291,\n",
      "          3370,  1012,   102,     0,     0,     0],\n",
      "        [  101,  3806, 12412, 13697,  3351,  2003,  2019,  4621,  3949,  2005,\n",
      "         11325, 11498, 16211,  2102, 16149,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101, 13822,  1997,  1042, 23518,  5648,  1006,  6904,  1007,  1998,\n",
      "         17663,  1038,  2575,  1006,  1058,  2497,  2575,  1007, 13416,  3798,\n",
      "          1997, 24004,  5666,  8602,  2063,  1012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19395, 19137,  5302, 15069,  3111,  2031,  1037,  2152,  3014,  1997,\n",
      "          3526,  1998,  8153,  3563, 19314,  1012,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 2012,  2546,  2549,  2003,  1037,  2236,  2203,  7361,  8523,  7712,\n",
      "          2128,  4588, 25100,  6911, 12115,  1012,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [18672,  8844, 26450, 11110,  2024,  2062,  4621,  2084,  2236,  6912,\n",
      "          7242,  1999,  8161,  3255,  1998,  9229,  3853,  1997,  1996,  3244,\n",
      "          1012,   102],\n",
      "        [ 6335,  1011,  1020, 14828,  3248,  1037,  2350,  2535,  1999,  2012,\n",
      "          5886,  2891, 14321, 21709,  2594, 22935,  4295,  1012,   102,     0,\n",
      "             0,     0]]) tensor([[  101, 19395, 19137,  5302, 15069,  3111,  2031,  1037,  2152,  3014,\n",
      "          1997,  3526,  1998,  8153,  3563, 19314,  1012,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2012,  2546,  2549,  2003,  1037,  2236,  2203,  7361,  8523,\n",
      "          7712,  2128,  4588, 25100,  6911, 12115,  1012,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 18672,  8844, 26450, 11110,  2024,  2062,  4621,  2084,  2236,\n",
      "          6912,  7242,  1999,  8161,  3255,  1998,  9229,  3853,  1997,  1996,\n",
      "          3244,  1012],\n",
      "        [  101,  6335,  1011,  1020, 14828,  3248,  1037,  2350,  2535,  1999,\n",
      "          2012,  5886,  2891, 14321, 21709,  2594, 22935,  4295,  1012,   102,\n",
      "             0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3465, 12353,  9312,  2015,  2241,  2006, 13675,  6593,  2951, 14125,\n",
      "          8339,  7597,  2005,  5022,  1999,  5025,  6612,  3218,  1012,   102,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 2512, 10536,  4842, 25808,  3512,  2111,  2040,  2024,  4583,  2086,\n",
      "          2214,  2031,  1037,  3938,  1003,  3382,  1997,  4975, 23760, 29048,\n",
      "          2076,  2037,  6480,  1012,   102],\n",
      "        [ 1054,  1011,  5250, 15775,  4842, 21821,  2024,  2179,  1999, 15420,\n",
      "          2007,  2037,  1054,  1011,  5250,  8031,  5826,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 2396,  2038,  2053,  3466,  2006,  1996,  1999, 25969,  3512,  2791,\n",
      "          1997,  9820,  1011,  3893,  2111,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]) tensor([[  101,  3465, 12353,  9312,  2015,  2241,  2006, 13675,  6593,  2951,\n",
      "         14125,  8339,  7597,  2005,  5022,  1999,  5025,  6612,  3218,  1012,\n",
      "           102,     0,     0,     0,     0],\n",
      "        [  101,  2512, 10536,  4842, 25808,  3512,  2111,  2040,  2024,  4583,\n",
      "          2086,  2214,  2031,  1037,  3938,  1003,  3382,  1997,  4975, 23760,\n",
      "         29048,  2076,  2037,  6480,  1012],\n",
      "        [  101,  1054,  1011,  5250, 15775,  4842, 21821,  2024,  2179,  1999,\n",
      "         15420,  2007,  2037,  1054,  1011,  5250,  8031,  5826,  1012,   102,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  2396,  2038,  2053,  3466,  2006,  1996,  1999, 25969,  3512,\n",
      "          2791,  1997,  9820,  1011,  3893,  2111,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3078, 28711, 22330, 23479, 11326,  2007,  6522,  2615, 10788,  2038,\n",
      "          3020, 20134, 14639,  2000, 11487,  5729, 28711, 26721, 13699,  8939,\n",
      "         24587,  9253, 24759, 15396,  2084,  7511, 22330, 23479,  1012,   102],\n",
      "        [ 3690,  2361,  2487,  1055, 16275,  2015,  2024,  4958, 11921,  4588,\n",
      "          2000,  1044,  2721,  2035, 26741,  2005,  2019,  4801, 10483,  2075,\n",
      "         11867, 15422,  8516, 13706,  1012,   102,     0,     0,     0,     0],\n",
      "        [ 1999,  5022,  2007, 27480,  2540, 18419,  1010,  1156,  1011,  3796,\n",
      "          2545,  6022,  5335,  6612,  2540,  4945, 13105,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 5423,  7361,  4747,  7274,  6305,  7507, 15637,  2015,  2031,  2019,\n",
      "         21733,  2981,  3466,  2006, 14234,  8803,  3853,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]) tensor([[  101,  3078, 28711, 22330, 23479, 11326,  2007,  6522,  2615, 10788,\n",
      "          2038,  3020, 20134, 14639,  2000, 11487,  5729, 28711, 26721, 13699,\n",
      "          8939, 24587,  9253, 24759, 15396,  2084,  7511, 22330, 23479,  1012],\n",
      "        [  101,  3690,  2361,  2487,  1055, 16275,  2015,  2024,  4958, 11921,\n",
      "          4588,  2000,  1044,  2721,  2035, 26741,  2005,  2019,  4801, 10483,\n",
      "          2075, 11867, 15422,  8516, 13706,  1012,   102,     0,     0,     0],\n",
      "        [  101,  1999,  5022,  2007, 27480,  2540, 18419,  1010,  1156,  1011,\n",
      "          3796,  2545,  6022,  5335,  6612,  2540,  4945, 13105,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5423,  7361,  4747,  7274,  6305,  7507, 15637,  2015,  2031,\n",
      "          2019, 21733,  2981,  3466,  2006, 14234,  8803,  3853,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27218,  1998, 10958,  2094,  2509,  3141,  5250,  2024,  4187,  2005,\n",
      "         13851,  6064,  4053,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 3938,  1003,  1997,  5573, 10527,  2331,  8715,  1006, 15765,  2015,\n",
      "          1007,  6677,  4148,  1999, 20662,  2015,  4793,  2625,  2084,  1020,\n",
      "          2706,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 2139, 10814,  3508,  1997,  1056,  1011,  2393,  2121,  2459,  1006,\n",
      "         16215, 16576,  1007,  4442,  2076, 28684,  2319, 10047, 23041, 10244,\n",
      "          8873, 29125,  7865,  1006,  9033,  2615,  1007,  8985, 17913, 28170,\n",
      "          1997, 11840,  8411,  5939, 21850, 20136,  5007,  2013,  1996,  9535,\n",
      "          1012,   102],\n",
      "        [ 1052,  2243,  2290,  1011,  2474,  2515,  2025,  2031,  1037,  2312,\n",
      "          4254,  2006,  3670,  1997, 16492,  2146,  2744, 16834, 18963,  1999,\n",
      "         18720,  2243,  1011,  2474, 11369, 12328,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]) tensor([[  101, 27218,  1998, 10958,  2094,  2509,  3141,  5250,  2024,  4187,\n",
      "          2005, 13851,  6064,  4053,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  3938,  1003,  1997,  5573, 10527,  2331,  8715,  1006, 15765,\n",
      "          2015,  1007,  6677,  4148,  1999, 20662,  2015,  4793,  2625,  2084,\n",
      "          1020,  2706,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2139, 10814,  3508,  1997,  1056,  1011,  2393,  2121,  2459,\n",
      "          1006, 16215, 16576,  1007,  4442,  2076, 28684,  2319, 10047, 23041,\n",
      "         10244,  8873, 29125,  7865,  1006,  9033,  2615,  1007,  8985, 17913,\n",
      "         28170,  1997, 11840,  8411,  5939, 21850, 20136,  5007,  2013,  1996,\n",
      "          9535,  1012],\n",
      "        [  101,  1052,  2243,  2290,  1011,  2474,  2515,  2025,  2031,  1037,\n",
      "          2312,  4254,  2006,  3670,  1997, 16492,  2146,  2744, 16834, 18963,\n",
      "          1999, 18720,  2243,  1011,  2474, 11369, 12328,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4013,  1011, 20187, 22330, 18715, 10586,  2024,  2039, 12222,  2076,\n",
      "         13656,  2458,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 1999,  7388,  4456,  1010,  1996,  3279,  1997,  2026,  8913, 23270,\n",
      "         16001,  4818,  4442, 14067,  1996,  6653,  1997, 23245,  2389,  2482,\n",
      "         21081,  2863,  1999, 26179,  2000, 17503,  2482, 21081,  2863,  1012,\n",
      "           102],\n",
      "        [ 2128, 25690,  2594,  5648, 10769,  1011,  3141, 18211, 10769, 13091,\n",
      "          1006, 20996,  2099, 29721,  1007,  2003,  1037, 17261,  4539,  2005,\n",
      "          3459,  8156,  1011, 13070, 25086,  4456,  1006, 13675, 15042,  1007,\n",
      "           102],\n",
      "        [ 2943,  5703,  5942,  1044, 22571, 14573,  7911,  7712,  1043,  7630,\n",
      "         28282,  2618, 11265, 10976,  6494,  3619, 25481,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]]) tensor([[  101,  4013,  1011, 20187, 22330, 18715, 10586,  2024,  2039, 12222,\n",
      "          2076, 13656,  2458,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  1999,  7388,  4456,  1010,  1996,  3279,  1997,  2026,  8913,\n",
      "         23270, 16001,  4818,  4442, 14067,  1996,  6653,  1997, 23245,  2389,\n",
      "          2482, 21081,  2863,  1999, 26179,  2000, 17503,  2482, 21081,  2863,\n",
      "          1012],\n",
      "        [  101,  2128, 25690,  2594,  5648, 10769,  1011,  3141, 18211, 10769,\n",
      "         13091,  1006, 20996,  2099, 29721,  1007,  2003,  1037, 17261,  4539,\n",
      "          2005,  3459,  8156,  1011, 13070, 25086,  4456,  1006, 13675, 15042,\n",
      "          1007],\n",
      "        [  101,  2943,  5703,  5942,  1044, 22571, 14573,  7911,  7712,  1043,\n",
      "          7630, 28282,  2618, 11265, 10976,  6494,  3619, 25481,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1041,  2290,  2581, 21434,  2064,  2022,  2730,  2011, 22575, 23060,\n",
      "         11475, 24759, 20363,  3539,  2094,  3729,  2620,  1009,  1056,  4442,\n",
      "          2013,  1996, 13656,  1011, 19689,  1048, 24335,  8458, 13045,  4442,\n",
      "          1012,   102],\n",
      "        [ 1037,  2531,  2290, 13004,  1997, 10381, 10626,  2080, 12519,  2063,\n",
      "         18178,  5302, 21572, 21281,  2721, 13306,  6939,  2078,  3463,  1999,\n",
      "          2128, 13770,  2140, 22423,  2044,  1015,  2095,  1997,  4882, 13822,\n",
      "          1012,   102],\n",
      "        [ 8247,  1011,  2316,  2522,  5886, 10127,  2003,  9412,  2005,  5710,\n",
      "         22239,  2058,  8841, 22239,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 5072,  4544, 16360, 25932,  9099, 23296, 13210, 22311,  3366,  1016,\n",
      "          4023,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]) tensor([[  101,  1041,  2290,  2581, 21434,  2064,  2022,  2730,  2011, 22575,\n",
      "         23060, 11475, 24759, 20363,  3539,  2094,  3729,  2620,  1009,  1056,\n",
      "          4442,  2013,  1996, 13656,  1011, 19689,  1048, 24335,  8458, 13045,\n",
      "          4442,  1012],\n",
      "        [  101,  1037,  2531,  2290, 13004,  1997, 10381, 10626,  2080, 12519,\n",
      "          2063, 18178,  5302, 21572, 21281,  2721, 13306,  6939,  2078,  3463,\n",
      "          1999,  2128, 13770,  2140, 22423,  2044,  1015,  2095,  1997,  4882,\n",
      "         13822,  1012],\n",
      "        [  101,  8247,  1011,  2316,  2522,  5886, 10127,  2003,  9412,  2005,\n",
      "          5710, 22239,  2058,  8841, 22239,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  5072,  4544, 16360, 25932,  9099, 23296, 13210, 22311,  3366,\n",
      "          1016,  4023,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2192,  2475, 25003,  3370,  2003,  1037,  3145,  3357,  1999,  2220,\n",
      "          2203,  8462, 18886,  2389,  2482, 21081, 23737,  1012,   102],\n",
      "        [ 7953,  2013,  5177,  1998,  3558,  2740,  2729,  8390,  2003,  2025,\n",
      "          4621,  2012, 16922, 11573,  2791,  1012,   102,     0,     0],\n",
      "        [11311,  3375, 13330,  3526,  2331,  5260,  2000,  4469, 16882,  2713,\n",
      "          1997,  4517,  6064,  1012,   102,     0,     0,     0,     0],\n",
      "        [19132,  2038,  1037,  2152,  9207,  4818,  3977,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]) tensor([[  101,  2192,  2475, 25003,  3370,  2003,  1037,  3145,  3357,  1999,\n",
      "          2220,  2203,  8462, 18886,  2389,  2482, 21081, 23737,  1012],\n",
      "        [  101,  7953,  2013,  5177,  1998,  3558,  2740,  2729,  8390,  2003,\n",
      "          2025,  4621,  2012, 16922, 11573,  2791,  1012,   102,     0],\n",
      "        [  101, 11311,  3375, 13330,  3526,  2331,  5260,  2000,  4469, 16882,\n",
      "          2713,  1997,  4517,  6064,  1012,   102,     0,     0,     0],\n",
      "        [  101, 19132,  2038,  1037,  2152,  9207,  4818,  3977,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "tensor([[22466,  4017,  7277,  5970,  5260,  2000,  4997, 13105,  1999,  5177,\n",
      "          2740,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1996,  3891,  1997,  4456,  9466,  2007,  2504,  1997,  6544,  8381,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [10551, 23725, 26402,  2033,  6499,  4063,  9067, 20582,  2011,  2139,\n",
      "         16307,  2075,  8247,  1011,  4937, 18595,  2078,  1999,  2019, 20014,\n",
      "         13910,  6657,  1011,  7790,  5450,  1012,   102],\n",
      "        [ 1050, 22022,  2620,  2072, 14494,  3426,  5012,  2000, 11265, 24093,\n",
      "         19265,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]) tensor([[  101, 22466,  4017,  7277,  5970,  5260,  2000,  4997, 13105,  1999,\n",
      "          5177,  2740,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  3891,  1997,  4456,  9466,  2007,  2504,  1997,  6544,\n",
      "          8381,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 10551, 23725, 26402,  2033,  6499,  4063,  9067, 20582,  2011,\n",
      "          2139, 16307,  2075,  8247,  1011,  4937, 18595,  2078,  1999,  2019,\n",
      "         20014, 13910,  6657,  1011,  7790,  5450,  1012],\n",
      "        [  101,  1050, 22022,  2620,  2072, 14494,  3426,  5012,  2000, 11265,\n",
      "         24093, 19265,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3558,  4023,  2504,  2003,  3378,  2007,  1996,  4489,  1999, 29160,\n",
      "          7722,  8381,  2090,  2304,  1998,  2317,  3360,  1012,   102,     0,\n",
      "             0],\n",
      "        [ 1996,  4013, 15509, 18514,  3977,  1997, 15756,  4013,  6914, 27287,\n",
      "         12980,  2408,  2427,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 3788,  1999,  4026,  2752,  1999,  2414,  2106,  2025,  5335, 11192,\n",
      "          3853,  1999,  9750,  6001,  1012,   102,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [14719, 11649, 16306,  2188, 28696,  6190,  2011, 16081,  2075,  2659,\n",
      "          1011,  2504,  1005,  1005, 17271,  2100,  1005,  1005, 14193,  1012,\n",
      "           102]]) tensor([[  101,  3558,  4023,  2504,  2003,  3378,  2007,  1996,  4489,  1999,\n",
      "         29160,  7722,  8381,  2090,  2304,  1998,  2317,  3360,  1012,   102,\n",
      "             0],\n",
      "        [  101,  1996,  4013, 15509, 18514,  3977,  1997, 15756,  4013,  6914,\n",
      "         27287, 12980,  2408,  2427,  1012,   102,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  3788,  1999,  4026,  2752,  1999,  2414,  2106,  2025,  5335,\n",
      "         11192,  3853,  1999,  9750,  6001,  1012,   102,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 14719, 11649, 16306,  2188, 28696,  6190,  2011, 16081,  2075,\n",
      "          2659,  1011,  2504,  1005,  1005, 17271,  2100,  1005,  1005, 14193,\n",
      "          1012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2852, 19137, 26083, 10514, 18098, 10732,  3363,  7934,  2552, 16940,\n",
      "         20049,  2078,  5090,  2024,  2179,  2012,  7372,  1999,  3358, 10047,\n",
      "         22974, 12032, 15303,  1012,   102],\n",
      "        [ 2493,  2040,  4685,  9996,  1999,  1996,  2220,  2086,  1997,  2966,\n",
      "          2082,  2024,  2012,  3445,  3891,  2005,  2658, 23337,  2101,  1999,\n",
      "          2037, 10922,  1012,   102,     0],\n",
      "        [ 4419,  2080,  2509,  2050, 13791,  1999, 11265, 21017,  2389,  2331,\n",
      "          2003, 26402,  2098,  2011, 22643,  7722,  2427,  1006, 20996,  2015,\n",
      "          1007,  1012,   102,     0,     0],\n",
      "        [ 9680, 24079, 15459,  7457,  1996,  6693,  1997, 13012, 15719,  2140,\n",
      "         25643, 17119, 27896,  1999,  5909, 10029,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0]]) tensor([[  101,  2852, 19137, 26083, 10514, 18098, 10732,  3363,  7934,  2552,\n",
      "         16940, 20049,  2078,  5090,  2024,  2179,  2012,  7372,  1999,  3358,\n",
      "         10047, 22974, 12032, 15303,  1012],\n",
      "        [  101,  2493,  2040,  4685,  9996,  1999,  1996,  2220,  2086,  1997,\n",
      "          2966,  2082,  2024,  2012,  3445,  3891,  2005,  2658, 23337,  2101,\n",
      "          1999,  2037, 10922,  1012,   102],\n",
      "        [  101,  4419,  2080,  2509,  2050, 13791,  1999, 11265, 21017,  2389,\n",
      "          2331,  2003, 26402,  2098,  2011, 22643,  7722,  2427,  1006, 20996,\n",
      "          2015,  1007,  1012,   102,     0],\n",
      "        [  101,  9680, 24079, 15459,  7457,  1996,  6693,  1997, 13012, 15719,\n",
      "          2140, 25643, 17119, 27896,  1999,  5909, 10029,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0]])\n",
      "tensor([[ 6583,  2278,  4078,  2696, 14454, 10057,  2053,  2000,  3623,  1996,\n",
      "          3466,  1997,  5688,  6074,  2006, 25125,  4972,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 1052, 16048, 19839,  2549,  2050, 16627,  2003,  5799,  2000,  2019,\n",
      "         19470,  6357,  3433,  3303,  2011,  1996, 12702,  2378, 12044,  3512,\n",
      "          3357,  1997,  3935,  8700,  9280, 16007, 27881, 22520,  1006,  6728,\n",
      "         19968,  2015,  1007,  1012,   102],\n",
      "        [ 8208,  1997,  1044,  2509,  2243,  2683,  4168,  2509, 24840, 16360,\n",
      "          3217, 13113,  6562,  8122,  1999,  2529,  2061, 12644,  3526,  4517,\n",
      "          4651,  7885,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [20582,  1997,  4013,  6914, 15660,  4442,  2000,  2026, 18349,  3593,\n",
      "          4442,  2003, 15315,  7974,  2098,  2043, 22597, 14828,  2003, 13712,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]]) tensor([[  101,  6583,  2278,  4078,  2696, 14454, 10057,  2053,  2000,  3623,\n",
      "          1996,  3466,  1997,  5688,  6074,  2006, 25125,  4972,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1052, 16048, 19839,  2549,  2050, 16627,  2003,  5799,  2000,\n",
      "          2019, 19470,  6357,  3433,  3303,  2011,  1996, 12702,  2378, 12044,\n",
      "          3512,  3357,  1997,  3935,  8700,  9280, 16007, 27881, 22520,  1006,\n",
      "          6728, 19968,  2015,  1007,  1012],\n",
      "        [  101,  8208,  1997,  1044,  2509,  2243,  2683,  4168,  2509, 24840,\n",
      "         16360,  3217, 13113,  6562,  8122,  1999,  2529,  2061, 12644,  3526,\n",
      "          4517,  4651,  7885,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101, 20582,  1997,  4013,  6914, 15660,  4442,  2000,  2026, 18349,\n",
      "          3593,  4442,  2003, 15315,  7974,  2098,  2043, 22597, 14828,  2003,\n",
      "         13712,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10851,  1996,  8290,  2090, 14595,  2361,  1011,  4724,  1998, 16464,\n",
      "          3375,  1045,  8171,  1050,  2094,  2509,  1998,  1050,  2094,  2575,\n",
      "         16263, 14595,  2361,  1011,  4724,  1011, 10572, 11265, 21017,  2389,\n",
      "          3279,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 3424,  7712,  3217, 21102,  6074,  2024,  2625,  4621,  2349,  2000,\n",
      "          1996,  3778,  1997,  3424,  7712,  3217, 21102,  8192,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 2146,  4677, 26572,  4609, 16846,  4648,  3064, 19101, 12737, 12448,\n",
      "          3370, 13416,  1059, 21030,  6774,  1998, 26180,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 2139, 10814,  3508,  1997,  1056,  1011,  2393,  2121,  2459,  1006,\n",
      "         16215, 16576,  1007,  4442,  2076, 28684,  2319, 10047, 23041, 10244,\n",
      "          8873, 29125,  7865,  1006,  9033,  2615,  1007,  8985,  7457, 28170,\n",
      "          1997, 11840,  8411,  5939, 21850, 20136,  5007,  2013,  1996,  9535,\n",
      "          1012,   102]]) tensor([[  101, 10851,  1996,  8290,  2090, 14595,  2361,  1011,  4724,  1998,\n",
      "         16464,  3375,  1045,  8171,  1050,  2094,  2509,  1998,  1050,  2094,\n",
      "          2575, 16263, 14595,  2361,  1011,  4724,  1011, 10572, 11265, 21017,\n",
      "          2389,  3279,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  3424,  7712,  3217, 21102,  6074,  2024,  2625,  4621,  2349,\n",
      "          2000,  1996,  3778,  1997,  3424,  7712,  3217, 21102,  8192,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2146,  4677, 26572,  4609, 16846,  4648,  3064, 19101, 12737,\n",
      "         12448,  3370, 13416,  1059, 21030,  6774,  1998, 26180,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2139, 10814,  3508,  1997,  1056,  1011,  2393,  2121,  2459,\n",
      "          1006, 16215, 16576,  1007,  4442,  2076, 28684,  2319, 10047, 23041,\n",
      "         10244,  8873, 29125,  7865,  1006,  9033,  2615,  1007,  8985,  7457,\n",
      "         28170,  1997, 11840,  8411,  5939, 21850, 20136,  5007,  2013,  1996,\n",
      "          9535,  1012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1044, 22571, 10085, 13465,  2378, 11265, 21017,  2229, 16081,  6634,\n",
      "         21572,  2638,  2110,  1999, 11432,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 2635,  4278, 24798,  1997,  1155,  1011,  2000,  3597, 27921,  8516,\n",
      "          9078, 12259,  1999,  5257,  2007, 17663,  1039, 13416,  1996,  3891,\n",
      "          1997, 25086,  4456,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [ 3891,  1997, 22935,  2824,  2064,  2022,  3013,  2011,  1037,  2353,\n",
      "          2011,  2478,  3424, 10536,  4842, 25808,  3512,  4319,  7242,  2426,\n",
      "         19610,  7716,  4818, 20960,  5022,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [26721,  1011,  8292,  2890, 12618, 15338,  7277,  7934,  1999, 20523,\n",
      "          1997,  6864, 27710,  1011,  1156, 19330, 14031, 16862,  7457,  3670,\n",
      "          1997, 10882, 21337, 22471,  2378,  2828,  1011,  3523,  5884,  1011,\n",
      "          4820,  5250,  1019, 28848,  1999, 12328,  5099,  6873, 26468,  2072,\n",
      "          1012,   102]]) tensor([[  101,  1044, 22571, 10085, 13465,  2378, 11265, 21017,  2229, 16081,\n",
      "          6634, 21572,  2638,  2110,  1999, 11432,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2635,  4278, 24798,  1997,  1155,  1011,  2000,  3597, 27921,\n",
      "          8516,  9078, 12259,  1999,  5257,  2007, 17663,  1039, 13416,  1996,\n",
      "          3891,  1997, 25086,  4456,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  3891,  1997, 22935,  2824,  2064,  2022,  3013,  2011,  1037,\n",
      "          2353,  2011,  2478,  3424, 10536,  4842, 25808,  3512,  4319,  7242,\n",
      "          2426, 19610,  7716,  4818, 20960,  5022,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 26721,  1011,  8292,  2890, 12618, 15338,  7277,  7934,  1999,\n",
      "         20523,  1997,  6864, 27710,  1011,  1156, 19330, 14031, 16862,  7457,\n",
      "          3670,  1997, 10882, 21337, 22471,  2378,  2828,  1011,  3523,  5884,\n",
      "          1011,  4820,  5250,  1019, 28848,  1999, 12328,  5099,  6873, 26468,\n",
      "          2072,  1012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2383,  1037,  2364,  4256,  4788,  3619,  9820, 13105,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [11707,  3949,  2003,  2025,  6020,  2000,  2512,  1011, 11707,  1999,\n",
      "         12318,  6001,  2007, 12936, 28929,  1997,  1996,  4013,  9048,  9067,\n",
      "         20368,  7946,  1012,   102],\n",
      "        [11311,  3375, 13330,  3526,  2331,  5260,  2000,  7367, 15500,  8156,\n",
      "          1997, 23060, 28173,  5422, 23079,  6064,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [ 1044,  2549,  1043,  2683,  2549,  2361,  8171, 26402, 10381, 21716,\n",
      "         20363,  3320,  1010,  2029, 17913,  2489,  2010, 11115,  1012,   102,\n",
      "             0,     0,     0,     0]]) tensor([[  101,  2383,  1037,  2364,  4256,  4788,  3619,  9820, 13105,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101, 11707,  3949,  2003,  2025,  6020,  2000,  2512,  1011, 11707,\n",
      "          1999, 12318,  6001,  2007, 12936, 28929,  1997,  1996,  4013,  9048,\n",
      "          9067, 20368,  7946,  1012],\n",
      "        [  101, 11311,  3375, 13330,  3526,  2331,  5260,  2000,  7367, 15500,\n",
      "          8156,  1997, 23060, 28173,  5422, 23079,  6064,  1012,   102,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [  101,  1044,  2549,  1043,  2683,  2549,  2361,  8171, 26402, 10381,\n",
      "         21716, 20363,  3320,  1010,  2029, 17913,  2489,  2010, 11115,  1012,\n",
      "           102,     0,     0,     0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1996, 16192,  2565, 16691,  2053,  3278,  4254,  2006, 16012, 15869,\n",
      "         13105,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [26402,  2075, 18423,  1011,  1020,  1011,  6887,  2891, 17585,  2139,\n",
      "         10536, 22196, 28835, 24840,  5423, 23924, 19009,  2011, 26709,  6593,\n",
      "         17441,  1048,  2243,  2497,  2487,  1011, 23713,  2243, 14828,  1012,\n",
      "           102],\n",
      "        [ 1051,  4017, 23691, 24873,  6632,  2278,  5022,  2089,  2031,  1051,\n",
      "          4017,  3563, 20187,  4442,  1999,  2037,  2235,  6812,  2884, 14163,\n",
      "         13186,  2050,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [ 2273,  2024,  2062, 18002,  2000,  2331,  2349,  2000, 18583,  2043,\n",
      "          4102,  2000,  2308,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]]) tensor([[  101,  1996, 16192,  2565, 16691,  2053,  3278,  4254,  2006, 16012,\n",
      "         15869, 13105,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101, 26402,  2075, 18423,  1011,  1020,  1011,  6887,  2891, 17585,\n",
      "          2139, 10536, 22196, 28835, 24840,  5423, 23924, 19009,  2011, 26709,\n",
      "          6593, 17441,  1048,  2243,  2497,  2487,  1011, 23713,  2243, 14828,\n",
      "          1012],\n",
      "        [  101,  1051,  4017, 23691, 24873,  6632,  2278,  5022,  2089,  2031,\n",
      "          1051,  4017,  3563, 20187,  4442,  1999,  2037,  2235,  6812,  2884,\n",
      "         14163, 13186,  2050,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2273,  2024,  2062, 18002,  2000,  2331,  2349,  2000, 18583,\n",
      "          2043,  4102,  2000,  2308,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[28157, 14828,  2013,  1996,  8040,  2546,  2615,  2003, 26986,  2011,\n",
      "          2026,  2094,  2620,  2620,  1013,  3729, 12740,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [18847, 20464, 16026, 27781, 14126,  1997,  1050,  1011, 28353,  5886,\n",
      "          2378, 16171, 18804,  9153,  6190,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [16480,  4244, 27833, 10578, 19653,  2015,  1047, 10270,  2549,  3670,\n",
      "          1999,  5443, 12458,  2015,  1010,  4525,  1999,  1996,  3670,  1997,\n",
      "          4013,  1011, 20187, 22330, 18715, 10586,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 6970,  7512,  2239, 25194,  9165,  1006,  2003,  5620,  1007, 20868,\n",
      "          2546,  2487,  1010, 20868,  2290,  2487,  1010,  2065,  2072, 22907,\n",
      "          1010,  1998, 12667,  4215,  2475,  8627,  2225, 15179,  7865, 21647,\n",
      "          1999,  2522, 28228,  9289, 15698,  1012,   102]]) tensor([[  101, 28157, 14828,  2013,  1996,  8040,  2546,  2615,  2003, 26986,\n",
      "          2011,  2026,  2094,  2620,  2620,  1013,  3729, 12740,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 18847, 20464, 16026, 27781, 14126,  1997,  1050,  1011, 28353,\n",
      "          5886,  2378, 16171, 18804,  9153,  6190,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 16480,  4244, 27833, 10578, 19653,  2015,  1047, 10270,  2549,\n",
      "          3670,  1999,  5443, 12458,  2015,  1010,  4525,  1999,  1996,  3670,\n",
      "          1997,  4013,  1011, 20187, 22330, 18715, 10586,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6970,  7512,  2239, 25194,  9165,  1006,  2003,  5620,  1007,\n",
      "         20868,  2546,  2487,  1010, 20868,  2290,  2487,  1010,  2065,  2072,\n",
      "         22907,  1010,  1998, 12667,  4215,  2475,  8627,  2225, 15179,  7865,\n",
      "         21647,  1999,  2522, 28228,  9289, 15698,  1012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6583,  2278, 26402,  2015,  1996,  4245,  1997, 17076, 25185,  6132,\n",
      "          2378,  1011, 16401,  9007,  1012,   102,     0],\n",
      "        [ 3768,  1997,  1042, 25708, 17465,  1999, 12328,  7457,  2166,  5987,\n",
      "         11656,  1012,   102,     0,     0,     0,     0],\n",
      "        [ 9680, 24079, 15459, 14350, 12520,  1999,  5909, 10029,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1996,  3891,  1997,  2931,  5895,  7386,  2075,  3209,  2003,  2702,\n",
      "          2335,  2008,  1997,  3287,  5895,  1012,   102]]) tensor([[  101,  6583,  2278, 26402,  2015,  1996,  4245,  1997, 17076, 25185,\n",
      "          6132,  2378,  1011, 16401,  9007,  1012,   102],\n",
      "        [  101,  3768,  1997,  1042, 25708, 17465,  1999, 12328,  7457,  2166,\n",
      "          5987, 11656,  1012,   102,     0,     0,     0],\n",
      "        [  101,  9680, 24079, 15459, 14350, 12520,  1999,  5909, 10029,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  3891,  1997,  2931,  5895,  7386,  2075,  3209,  2003,\n",
      "          2702,  2335,  2008,  1997,  3287,  5895,  1012]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   8%|▊         | 17/202 [00:04<00:50,  3.64it/s]\n",
      "Epoch:   0%|          | 0/1 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluator=dev_evaluator,\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluation_steps=evaluation_steps,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# output_path=model_save_path,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True, if your GPU supports FP16 cores\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/sentence_transformers/SentenceTransformer.py:965\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    963\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    964\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(loss_model\u001b[38;5;241m.\u001b[39mparameters(), max_grad_norm)\n\u001b[0;32m--> 965\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    968\u001b[0m skip_scheduler \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale() \u001b[38;5;241m!=\u001b[39m scale_before_step\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:378\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke ``unscale_(optimizer)`` followed by parameter update, if gradients are not infs/NaN.\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enabled:\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosure\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/optim/adamw.py:176\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    173\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    174\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 176\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     adamw(\n\u001b[1;32m    188\u001b[0m         params_with_grad,\n\u001b[1;32m    189\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/optim/adamw.py:111\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdamW does not support sparse gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m grads\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m--> 111\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# State initialization\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# note(crcrpar): Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/torch/_tensor.py:1034\u001b[0m, in \u001b[0;36mTensor.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1025\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1026\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1030\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1031\u001b[0m         )\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m-> 1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;66;03m# subclass if needed.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    # evaluator=dev_evaluator,\n",
    "    epochs=1,\n",
    "    # evaluation_steps=evaluation_steps,\n",
    "    # output_path=model_save_path,\n",
    "    weight_decay=0,\n",
    "    warmup_steps=100,\n",
    "    optimizer_params={\"lr\": 3e-5},\n",
    "    use_amp=True,  # Set to True, if your GPU supports FP16 cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[MASK]dasdf ads[MASK] sda [MASK]ds [MASK]asd gasdg aasdfa[MASK]f /dsaf sadfsd,dasfa[MASK]f. [MASK]ds[MASK].fads,fadsfa',\n",
       " '[MASK]dasdf [MASK][MASK] sda fads [MASK]asd gas[MASK]g [MASK]sd[MASK][MASK][MASK] [MASK][MASK]af [MASK][MASK][MASK][MASK]dasfadsf[MASK] [MASK][MASK][MASK].fads[MASK]fads[MASK]',\n",
       " \"asdasdf adsf sda fads 'asd gasdg aasdfasdf /dsaf sadfsd,dasfadsf. fadsf.fads,fadsfa\"]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(MaskedAutoEncoderDataset([\"asdasdf adsf sda fads 'asd gasdg aasdfasdf /dsaf sadfsd,dasfadsf. fadsf.fads,fadsfa\"],tokenizer))).texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"asdasdf adsf sda fads ' asd gasdg aasdfasdf / dsaf sadfsd, dasfadsf. fadsf. fads, fadsfa\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"asdasdf adsf sda fads 'asd gasdg aasdfasdf /dsaf sadfsd,dasfadsf. fadsf.fads,fadsfa\"\n",
    "tokenizer.decode(tokenizer.encode_plus(sentence,return_attention_mask=False,return_token_type_ids=False,add_special_tokens=False)['input_ids'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as [MASK] df'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode('as [MASK]df',add_special_tokens=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/cgrdj/nltk_data'\n    - '/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/nltk_data'\n    - '/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/share/nltk_data'\n    - '/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m masked_sentences\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 4\u001b[0m     words\u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Apply the masking logic to each word and rejoin the sentence\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     splitted_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(words,return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/Documents/code/repos/sentence-transformers/.conda/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/cgrdj/nltk_data'\n    - '/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/nltk_data'\n    - '/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/share/nltk_data'\n    - '/home/cgrdj/Documents/code/repos/sentence-transformers/.conda/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences=[\"asdasdf adsf sda fads 'asd gasdg aasdfasdf /dsaf sadfsd,dasfadsf. fadsf.fads,fadsfa\"]*10000\n",
    "masked_sentences=[]\n",
    "for sentence in sentences:\n",
    "    words= word_tokenize(sentence)\n",
    "    # Apply the masking logic to each word and rejoin the sentence\n",
    "    splitted_tokens = tokenizer.batch_encode_plus(words,return_attention_mask=False,return_token_type_ids=False,add_special_tokens=False)['input_ids']\n",
    "    masked_sentence=' '.join([tokenizer.decode([ mask_id if np.random.rand() < mask_probability else tok_id for tok_id in word]).replace(\" \",'') for word in splitted_tokens])\n",
    "    masked_sentences.append(masked_sentence)\n",
    "\n",
    "\n",
    "\n",
    "# masked_sentence = ' '.join([mask_token if np.random.rand() < mask_probability else word for word in words])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6904, 5104, 2546, 1012, 6904, 5104]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_spaces_table=str.maketrans('', '', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.5 µs ± 2.3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "tokenizer.decode([ mask_id if np.random.rand() < mask_probability else tok_id for tok_id in splitted_tokens[12]]).replace(\" \",'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.8 µs ± 4.79 µs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100000\n",
    "tokenizer.decode([ mask_id if np.random.rand() < mask_probability else tok_id for tok_id in splitted_tokens[12]]).translate(remove_spaces_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1042, 102], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'tokens': 'int' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/sentence-transformers/.venv/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:612\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_tokens_to_string\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'tokens': 'int' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokenizer(sentence)['input_ids'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"asdas [MASK] adsf sda fads ' asd gas [MASK]g aasdfasdf / dsaf sad [MASK]d, [MASK]fadsf. fadsf. fa [MASK], fadsfa\""
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_probability=0.15\n",
    "mask_token = tokenizer.mask_token  # Get the mask token\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "# Decide randomly which tokens to mask\n",
    "masked_indices = np.random.rand(len(tokens)) < mask_probability\n",
    "# Replace selected tokens with the mask token\n",
    "masked_tokens = [mask_token if mask else token for token, mask in zip(tokens, masked_indices)]\n",
    "# Convert the list of tokens back to a string\n",
    "masked_sentence = tokenizer.convert_tokens_to_string(masked_tokens)\n",
    "# Add the masked sentence to the list\n",
    "# masked_sentences.append(masked_sentence)\n",
    "masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as [MASK]df adsf sd [MASK] fads ' asd gas [MASK]g aa [MASK]fasdf [MASK] [MASK] [MASK] sadfsd, dasfadsf. [MASK] [MASK] [MASK]. fads [MASK] fa [MASK]fa\""
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdasdf',\n",
       " 'adsf',\n",
       " 'sda',\n",
       " 'fads',\n",
       " \"'asd\",\n",
       " 'gasdg',\n",
       " 'aasdfasdf',\n",
       " '/dsaf',\n",
       " 'sadfsd',\n",
       " ',',\n",
       " 'dasfadsf',\n",
       " '.',\n",
       " 'fadsf.fads',\n",
       " ',',\n",
       " 'fadsfa']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word_tokenize(sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded here: /Users/cgrdj/Documents/Code/sentence-transformers/datasets/scifact\n"
     ]
    }
   ],
   "source": [
    "dataset = \"scifact\"\n",
    "url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset)\n",
    "out_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "data_path = util.download_and_unzip(url, out_dir)\n",
    "print(\"Dataset downloaded here: {}\".format(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 00:49:24 - Loading Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 11485.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 00:49:24 - Loaded 5183 TRAIN Documents.\n",
      "2024-03-16 00:49:24 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-03-16 00:49:24 - Loading Queries...\n",
      "2024-03-16 00:49:24 - Loaded 809 TRAIN Queries.\n",
      "2024-03-16 00:49:24 - Query Example: 0-dimensional biomaterials lack inductive properties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"train\") # or split = \"train\" or \"dev\"\n",
    "unsupervised_train_data =  list(queries.values())#+[data['title']+' \\n '+data['text'] for data in list(corpus.values())]\n",
    "random.Random(0).shuffle(unsupervised_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 00:49:25 - Loading Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5183 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5183/5183 [00:00<00:00, 16216.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-16 00:49:26 - Loaded 5183 TEST Documents.\n",
      "2024-03-16 00:49:26 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-03-16 00:49:26 - Loading Queries...\n",
      "2024-03-16 00:49:26 - Loaded 300 TEST Queries.\n",
      "2024-03-16 00:49:26 - Query Example: 0-dimensional biomaterials show inductive properties.\n"
     ]
    }
   ],
   "source": [
    "data_path = \"datasets/scifact\"\n",
    "test_corpus, test_queries, test_qrels = GenericDataLoader(data_path).load(split=\"test\") # or split = \"train\" or \"dev\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
